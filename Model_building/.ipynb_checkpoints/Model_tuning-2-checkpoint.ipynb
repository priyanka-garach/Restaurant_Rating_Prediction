{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'setup_logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8086a0f2a24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSplitting_Scaling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'setup_logger'"
     ]
    }
   ],
   "source": [
    "from application_logger import setup_logger\n",
    "from Splitting_Scaling import *\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.ensemble import  ExtraTreesRegressor\n",
    "from setup_logger import setup_logger\n",
    "\n",
    "\n",
    "class Parameter_tuning:\n",
    "    def __init__(self):\n",
    "        self.folder = '../logs/'\n",
    "        self.filename = 'Model_tuning.txt'\n",
    "        if not os.path.isdir(self.folder):\n",
    "            os.mkdir(self.folder)\n",
    "        self.log_object = setup_logger(\"Model_tuning\",self.folder+self.filename)\n",
    "        \n",
    "        self.log_object.info('Strated calling Splitting_Scaling file')        \n",
    "        self.split_obj=Splitting_And_Scaling()\n",
    "        self.dict={}\n",
    "        self.log_object.info('Splitting_Scaling file called Sucessfully.')    \n",
    "        \n",
    "        \n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Method: parameters\n",
    "        Description: This method is used to define the parameters for the model\n",
    "        Parameters: None\n",
    "        Return: parameters for individual models\n",
    "\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "        self.log_object.info('Trying to set hyper-paramerts')  \n",
    "        et_parameters={'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1000, num = 12)],\n",
    "                       'max_features': ['auto', 'sqrt'],\n",
    "                       'max_depth': [int(x) for x in np.linspace(5, 30, num = 6)],\n",
    "                       'min_samples_split': [2, 5, 10],\n",
    "                       'min_samples_leaf': [1, 2, 5]\n",
    "                      }\n",
    "        \n",
    "        rf_parameters={'n_estimators': [100,200,300,400,500,600],\n",
    "                       'max_features': ['auto', 'sqrt'],\n",
    "                       'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                       'min_samples_split': [2, 5,10],\n",
    "                       'min_samples_leaf': [1, 2,4],\n",
    "                       'bootstrap': [True, False]\n",
    "                      }\n",
    "        self.log_object.info('Hyper-paramerts is successfully set.') \n",
    "        return et_parameters, rf_parameters\n",
    "        \n",
    "    def et_tuning(self):\n",
    "        \"\"\"\n",
    "        Method: et_tuning\n",
    "        Description: This method is used to tune the parameters for the Extra Trees Regressor model\n",
    "        Parameters: None\n",
    "        Return: Best hyperparameters for the Extra Trees Regressor model and tuned model\n",
    "\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "        self.log_object.info('Train-Test Split') \n",
    "        x_train,x_test,y_train,y_test=self.split_obj.scaling()\n",
    "        try:\n",
    "            self.log_object.info('ExtraTree Regressor: Model Tuning Started') \n",
    "            et_parameters = self.parameters()[0]\n",
    "            et_reg = ExtraTreesRegressor()\n",
    "            random_et = RandomizedSearchCV(estimator=et_reg,\n",
    "                                        param_distributions=et_parameters,\n",
    "                                        cv=5,\n",
    "                                        scoring='neg_root_mean_squared_error',\n",
    "                                        n_iter=10,\n",
    "                                        n_jobs=1,\n",
    "                                        verbose=2,\n",
    "                                        random_state=42\n",
    "                                        )\n",
    "            \n",
    "            random_et.fit(x_train,y_train) \n",
    "            best_param = random_et.best_params_\n",
    "            self.log_object.info('ExtraTree Regressor:Best Parameters found.') \n",
    "            \n",
    "            et_model = ExtraTreesRegressor(n_estimators = best_param['n_estimators'],\n",
    "                                            max_features = best_param['max_features'],\n",
    "                                            max_depth = best_param['max_depth'],\n",
    "                                            min_samples_split = best_param['min_samples_split'],\n",
    "                                            min_samples_leaf = best_param['min_samples_leaf'])\n",
    "            \n",
    "            self.log_object.info('ExtraTree Regressor:Using best parameter model tuning done.') \n",
    "            \n",
    "            et_model.fit(x_train,y_train)\n",
    "            self.log_object.info('ExtraTree Regressor:Training data fitted to tuned model.') \n",
    "        except Exception as e:\n",
    "            self.log_object.info('Error in ExtraTree Regressor Tuning.'+str(e)) \n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            self.log_object.info('ExtraTree Regressor:Finding train and test accuracy')\n",
    "            et_train_score=et_model.score(x_train,y_train)\n",
    "            et_test_score=et_model.score(x_test,y_test)\n",
    "            \n",
    "            y_pred = et_model.predict(x_test)\n",
    "            et_r2score=r2_score(y_test,y_pred)\n",
    "            self.log_object.info('ExtraTree Regressor:train and test accuracy found sucessfully.')\n",
    "            \n",
    "            self.log_object.info('ExtraTree Regressor:Converting accuracy scores to dictionary')\n",
    "            self.dict['Extratree']=[et_model,et_train_score,et_test_score,et_r2score]\n",
    "            self.log_object.info('ExtraTree Regressor:Succesfully converted accuracy scores to dictionary')\n",
    "        except Exception as e:\n",
    "            self.log_object.info('Error in ExtraTree Regressor Accuracy.'+str(e)) \n",
    "            raise e\n",
    "                \n",
    "    def rf_tuning(self):\n",
    "        \"\"\"\n",
    "        Method: rf_tuning\n",
    "        Description: This method is used to tune the parameters for the Random forest model\n",
    "        Parameters: None\n",
    "        Return: Best hyperparameters for the Random forest model and tuned model\n",
    "\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            rf_parameters = self.parameters()[1]\n",
    "            x_train, x_test, y_train, y_test = self.split_obj.scaling()\n",
    "            self.log_object.info('RandomForestRegressor: Train-Test Split') \n",
    "            \n",
    "            rf_reg = RandomForestRegressor()\n",
    "            self.log_object.info('RandomForestRegressor: Model Tuning Started') \n",
    "            \n",
    "            random_rf = RandomizedSearchCV(estimator=rf_reg,\n",
    "                                        param_distributions=rf_parameters,\n",
    "                                        cv=5,\n",
    "                                        scoring='neg_root_mean_squared_error',\n",
    "                                        n_iter=10,\n",
    "                                        n_jobs=1,\n",
    "                                        verbose=2,\n",
    "                                        random_state=45\n",
    "                                        )\n",
    "            random_rf.fit(x_train,y_train)\n",
    "            best_param = random_rf.best_params_\n",
    "            self.log_object.info('RandomForestRegressor: Best parameter found.') \n",
    "    \n",
    "            rf_model = RandomForestRegressor(n_estimators = best_param['n_estimators'],\n",
    "                                            min_samples_split = best_param['min_samples_split'],\n",
    "                                            min_samples_leaf = best_param['min_samples_leaf'],\n",
    "                                            max_features = best_param['max_features'],\n",
    "                                            max_depth = best_param['max_depth'],\n",
    "                                            bootstrap=best_param['bootstrap'])\n",
    "           \n",
    "            self.log_object.info('RandomForestRegressor: trying to fit model using best params') \n",
    "            rf_model.fit(x_train,y_train)\n",
    "            self.log_object.info('RandomForestRegressor: Success-fit model using best params') \n",
    "            \n",
    "            self.log_object.info('RandomForestRegressor:Finding train and test accuracy')\n",
    "            rf_train_score=rf_model.score(x_train,y_train)\n",
    "            rf_test_score=rf_model.score(x_test,y_test)\n",
    "            \n",
    "            y_pred = rf_model.predict(x_test)\n",
    "            rf_r2score=r2_score(y_test,y_pred)\n",
    "            self.log_object.info('RandomForestRegressor:train and test accuracy found sucessfully.')\n",
    "            \n",
    "            self.log_object.info('RandomForestRegressor:Converting accuracy scores to dictionary')\n",
    "            self.dict['Extratree_Model']=[rf_model,rf_train_score,rf_test_score,rf_r2score]\n",
    "            self.log_object.info('RandomForestRegressor:Succesfully converted accuracy scores to dictionary')\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log_object.info('Error in RandomForest Regressor Accuracy.'+str(e)) \n",
    "            raise e\n",
    "            \n",
    "            \n",
    "    def algo_run(self):\n",
    "        self.et_tuning()\n",
    "        self.rf_tuning()\n",
    "        \n",
    "    def model_result(self):\n",
    "        \"\"\"\n",
    "        Method: model_result\n",
    "        Description: This method is used to print the best model and the corresponding score\n",
    "        Parameters: None\n",
    "        Return: Store the scores obtain from different algorithms in a dictionary\n",
    "\n",
    "        Version: 1.0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.log_object.info('Findind best model..')\n",
    "            self.algo_run()\n",
    "            d = self.dict\n",
    "        \n",
    "            d = sorted(d.items(), key=lambda a:a[1][1])\n",
    "            \n",
    "            best_model_name = d[0][0]\n",
    "            best_model_object = d[0][1][0]\n",
    "            best_model_train_score = d[0][1][1]\n",
    "            best_model_test_score = d[0][1][2]\n",
    "            best_re2_score=d[0][1][3]\n",
    "            if not os.path.isdir('./bestmodel/'):\n",
    "                os.mkdir('./bestmodel/')\n",
    "            \n",
    "            with open('./bestmodel/'+best_model_name+'.pkl','wb') as file:\n",
    "                pickle.dump(best_model_object,file)\n",
    "#             return f'''Best model:{best_model_name}\n",
    "#                        Train_score:{best_model_train_score}\n",
    "#                        Test_score:{best_model_test_score}\n",
    "#                        R2_score:{best_re2_score}''' \n",
    "            print(f'Best model:{best_model_name}\\nTrain_score:{best_model_train_score}\\nTest_score:{best_model_test_score}\\nR2_score:{best_re2_score}')\n",
    "            self.log_object.info('Best model found.')\n",
    "        except Exception as e:\n",
    "            self.log_object.info('Error in finding best model'+str(e))\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = Parameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "o.model_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "95-86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
